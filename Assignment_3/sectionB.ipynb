{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "\n",
    "layer_sizes = [784, 256, 128, 64, 32, 10]\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, activation_func, activation_derivative, weight_init_func, N=5, layer_sizes=[784, 256, 128, 64, 32, 10], lr=0.01, epochs=100, batch_size=128):\n",
    "        self.N = N   \n",
    "        self.layer_sizes = layer_sizes \n",
    "        self.lr = lr  \n",
    "        self.activation_func = activation_func \n",
    "        self.activation_derivative = activation_derivative \n",
    "        self.weight_init_func = weight_init_func \n",
    "        self.epochs = epochs \n",
    "        self.batch_size = batch_size \n",
    "        self.weights = []\n",
    "        self.biases = [] \n",
    " \n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def fit(self, X, Y, X_val, Y_val):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs): \n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X, Y = X[indices], Y[indices]\n",
    " \n",
    "            batch_losses = []\n",
    "            for i in range(0, X.shape[0], self.batch_size):\n",
    "                X_batch = X[i:i + self.batch_size]\n",
    "                Y_batch = Y[i:i + self.batch_size]\n",
    "                 \n",
    "                activations, z_values = self.forward(X_batch)\n",
    "                loss = self.cross_entropy_loss(Y_batch, activations[-1])\n",
    "                batch_losses.append(loss)\n",
    " \n",
    "                weight_grads, bias_grads = self.backpropagate(activations, z_values, Y_batch)\n",
    " \n",
    "                for j in range(len(self.weights)):\n",
    "                    self.weights[j] -= self.lr * weight_grads[j]\n",
    "                    self.biases[j] -= self.lr * bias_grads[j]\n",
    "\n",
    "            train_losses.append(np.mean(batch_losses))\n",
    " \n",
    "            val_activations, _ = self.forward(X_val)\n",
    "            val_loss = self.cross_entropy_loss(Y_val, val_activations[-1])\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # print(f\"Epoch {epoch+1}/{self.epochs}, Training Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}\")\n",
    "\n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred): \n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))\n",
    "    \n",
    "    def initialize_weights(self): \n",
    "        for i in range(1, self.N):\n",
    "            self.weights.append(self.weight_init_func((self.layer_sizes[i-1], self.layer_sizes[i])))\n",
    "            self.biases.append(np.zeros((1, self.layer_sizes[i])))\n",
    "\n",
    "    def forward(self, X): \n",
    "        activ = [X]\n",
    "        D = []\n",
    " \n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activ[-1], self.weights[i]) + self.biases[i]\n",
    "            D.append(z)\n",
    "            activ.append(self.activation_func(z))\n",
    " \n",
    "        z = np.dot(activ[-1], self.weights[-1]) + self.biases[-1]\n",
    "        activ.append(self.softmax(z))\n",
    "        D.append(z)\n",
    "\n",
    "        return activ, D\n",
    "\n",
    "    def backpropagate(self, activations, z_values, y_true): \n",
    "        biasGrad = [np.zeros_like(b) for b in self.biases] \n",
    "        weightGrad = [np.zeros_like(w) for w in self.weights]\n",
    "        \n",
    "        delta = activations[-1] - y_true\n",
    "        weightGrad[-1] = np.dot(activations[-2].T, delta) / y_true.shape[0]\n",
    "        biasGrad[-1] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_derivative(z_values[i])\n",
    "            \n",
    "            weightGrad[i] = np.dot(activations[i].T, delta) / y_true.shape[0]\n",
    "            biasGrad[i] = np.sum(delta, axis=0, keepdims=True) / y_true.shape[0]\n",
    "            \n",
    "        return weightGrad, biasGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(48000, 784)\n",
      "(6000, 784)\n",
      "(6000, 784)\n",
      "(48000, 10)\n",
      "(6000, 10)\n",
      "(6000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def returnDataset():\n",
    "    def load_Lab(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            _ = struct.unpack(\">II\", f.read(8))\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "    def load_Im(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            _ = struct.unpack(\">IIII\", f.read(16))\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8).reshape(-1, 28*28) / 255.0\n",
    "        return data\n",
    "\n",
    "    train_images_path = '/Users/shamiksinha/Desktop/MLassign3/train-images-idx3-ubyte/train-images-idx3-ubyte'\n",
    "    train_labels_path = '/Users/shamiksinha/Desktop/MLassign3/train-labels-idx1-ubyte/train-labels-idx1-ubyte'\n",
    "    X_train = load_Im(train_images_path)\n",
    "    y_train = load_Lab(train_labels_path)\n",
    "    \n",
    "    test_images_path = '/Users/shamiksinha/Desktop/MLassign3/test-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "    test_labels_path = '/Users/shamiksinha/Desktop/MLassign3/test-labels-idx1-ubyte/t10k-labels-idx1-ubyte'\n",
    "    X_test = load_Im(test_images_path)\n",
    "    y_test = load_Lab(test_labels_path)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_val = encoder.transform(y_val.reshape(-1, 1))\n",
    "    y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = returnDataset()\n",
    "\n",
    "print(X_train.shape) \n",
    "print(X_test.shape) \n",
    "print(X_val.shape) \n",
    "print(y_train.shape) \n",
    "print(y_test.shape) \n",
    "print(y_val.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x): \n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x): \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x): \n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01): \n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01): \n",
    "    dx = np.ones_like(x); dx[x < 0] = alpha; return dx\n",
    "\n",
    "def tanh(x): \n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x): \n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(shape): \n",
    "    return np.zeros(shape)\n",
    "\n",
    "def random_init(shape): \n",
    "    return np.random.uniform(-1/np.sqrt(shape[0]), 1/np.sqrt(shape[0]), shape)\n",
    "\n",
    "def normal_init(shape):\n",
    "    return np.random.normal(0, np.sqrt(2/shape[0]), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = { \"sigmoid\": (sigmoid, sigmoid_derivative), \"relu\": (relu, relu_derivative), \"leaky_relu\": (leaky_relu, leaky_relu_derivative), \"tanh\": (tanh, tanh_derivative)}\n",
    "initializations = { \"random_init\": random_init, \"zero_init\": zero_init, \"normal_init\": normal_init }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(6000, 784)\n",
      "(6000, 784)\n",
      "(48000, 10)\n",
      "(6000, 10)\n",
      "(6000, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test\u001b[38;5;241m.\u001b[39mshape) \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_val\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m---> 20\u001b[0m trainLosses, valLosses \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, X_val, y_val)\n\u001b[1;32m     21\u001b[0m testPredictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     22\u001b[0m testLabels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 42\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X, Y, X_val, Y_val)\u001b[0m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_loss(Y_batch, activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     40\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m---> 42\u001b[0m weight_grads, bias_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagate(activations, z_values, Y_batch)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[j] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m weight_grads[j]\n",
      "Cell \u001b[0;32mIn[67], line 108\u001b[0m, in \u001b[0;36mNeuralNetwork.backpropagate\u001b[0;34m(self, activations, z_values, y_true)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    106\u001b[0m     delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_derivative(z_values[i])\n\u001b[0;32m--> 108\u001b[0m     weightGrad[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(activations[i]\u001b[38;5;241m.\u001b[39mT, delta) \u001b[38;5;241m/\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    109\u001b[0m     biasGrad[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(delta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weightGrad, biasGrad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    " \n",
    "for init_name, init_func in initializations.items():\n",
    "    for act_name, (activation_func, activation_derivative) in activations.items(): \n",
    "         \n",
    "        model = NeuralNetwork( \n",
    "            N=len(layer_sizes),\n",
    "            activation_func=activation_func,\n",
    "            activation_derivative=activation_derivative,\n",
    "            weight_init_func=init_func,  \n",
    "        )\n",
    "        \n",
    "        print(X_train.shape) \n",
    "        print(X_test.shape) \n",
    "        print(X_val.shape) \n",
    "        print(y_train.shape) \n",
    "        print(y_test.shape) \n",
    "        print(y_val.shape) \n",
    " \n",
    "        trainLosses, valLosses = model.fit(X_train, y_train, X_val, y_val)\n",
    "        testPredictions = model.predict(X_test)\n",
    "        testLabels = np.argmax(y_test, axis=1)\n",
    "        testAccuracy = np.score(testPredictions, testLabels)\n",
    " \n",
    "        results[f\"{init_name}_{act_name}\"] = {\n",
    "            \"val_losses\": valLosses,\n",
    "            \"train_losses\": trainLosses,\n",
    "            \"test_accuracy\": testAccuracy\n",
    "        }\n",
    "\n",
    "        filename = f\"model_{init_name}_{act_name}.pkl\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for init_name in initializations.keys():\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for act_name in activations.keys():\n",
    "        combo_key = f\"{init_name}_{act_name}\"\n",
    "        if combo_key in results: \n",
    "            plt.plot(results[combo_key][\"train_losses\"], label=f\"{act_name} - Train Loss\")\n",
    "            plt.plot(results[combo_key][\"val_losses\"], linestyle=\"--\", label=f\"{act_name} - Val Loss\")\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.title(f'Training and Validation Loss for {init_name} Initialization')\n",
    "    plt.show()\n",
    "\n",
    "for combo, metrics in results.items():\n",
    "    print(f\"{combo.capitalize()} Test Accuracy: {metrics['test_accuracy'] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"]\n",
    "initializations = [\"zero_init\", \"random_init\", \"normal_init\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for sigmoid with zero_init: 10.67%\n",
      "Test Accuracy for tanh with zero_init: 10.67%\n",
      "Test Accuracy for relu with zero_init: 10.67%\n",
      "Test Accuracy for leaky_relu with zero_init: 10.67%\n",
      "Test Accuracy for sigmoid with random_init: 10.67%\n",
      "Test Accuracy for tanh with random_init: 96.85%\n",
      "Test Accuracy for relu with random_init: 97.22%\n",
      "Test Accuracy for leaky_relu with random_init: 96.80%\n",
      "Test Accuracy for sigmoid with normal_init: 87.30%\n",
      "Test Accuracy for tanh with normal_init: 97.25%\n",
      "Test Accuracy for relu with normal_init: 97.38%\n",
      "Test Accuracy for leaky_relu with normal_init: 97.23%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    " \n",
    "for init in initializations:\n",
    "    for act in activations:\n",
    "        filename = f\"model_{init}_{act}.pkl\"\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                model = pickle.load(f) \n",
    "            test_pred = model.predict(X_test)\n",
    "            test_lab = np.argmax(y_test, axis=1)\n",
    "            test_acc = np.score(test_pred, test_lab)\n",
    "            print(f\"Test Accuracy : {act} with {init}: {test_acc * 100:.2f}%\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
